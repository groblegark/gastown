formula = "mol-gastown-e2e-full"
description = """
Gastown Full E2E Test Suite — Provisions an ephemeral K8s namespace, validates
the entire gastown stack, and tears it down on completion.

This is the single formula an agent runs to execute the complete E2E lifecycle:
  gt formula run mol-gastown-e2e-full

Each run spins up a fresh namespace (gastown-e2e-<timestamp>), installs the
gastown helm chart with all components enabled, runs the full test suite
(infrastructure + agent capabilities + Playwright), and cleans up.

Steps:
  0. Preflight — verify cluster access, helm, test scripts
  1. Provision — helm install into fresh namespace, wait for pods
  2. Seed — inject deploy config and wait for daemon to connect
  3. Infrastructure health — dolt, redis, daemon, nats, broker, controller, s3-sync, failsafe, slack-bot
  4. Agent capabilities — spawn, state, I/O, credentials, lifecycle, resume, multi, coordination, cleanup
  5. Mux dashboard — Playwright browser tests against coop-broker mux UI
  6. Summary — aggregate results, file bugs for failures
  7. Teardown — helm uninstall, delete namespace, verify cleanup

Failures auto-file bugs linked to the E2E test epic.
"""
version = 1
type = "workflow"

# ── Steps ────────────────────────────────────────────────────────────

[[steps]]
id = "preflight"
title = "Preflight checks"
description = """
Verify prerequisites before provisioning the E2E namespace.

Action:
  1. Verify kubectl can reach the cluster:
     kubectl cluster-info
  2. Verify helm is available:
     helm version
  3. Verify the gastown helm chart exists:
     ls helm/gastown/Chart.yaml
  4. Verify the E2E values overlay exists:
     ls helm/gastown/values-e2e.yaml
  5. Verify test scripts exist:
     ls tests/e2e/scripts/run-suite.sh tests/e2e/scripts/lib.sh
  6. Verify ExternalSecretStore is available (ClusterSecretStore aws-secretsmanager):
     kubectl get clustersecretstore aws-secretsmanager -o name
  7. If running Playwright tests, verify npx is available:
     command -v npx

Verify:
  All checks pass. Cluster is reachable, chart exists, secret store is configured.

OnFail:
  Cannot proceed. Check kubeconfig, VPN, helm installation, and chart paths.
"""

[[steps]]
id = "provision"
title = "Provision ephemeral E2E namespace"
description = """
Create a fresh K8s namespace and install the full gastown stack via helm.
The namespace name should be unique per run: gastown-e2e-$(date +%s)

Action:
  Generate a unique namespace name:
    E2E_NS="gastown-e2e-$(date +%s)"

  Run the provision script:
    ./tests/e2e/scripts/provision-namespace.sh \
      --namespace "$E2E_NS" \
      --chart-dir helm/gastown \
      --values helm/gastown/values-e2e.yaml \
      --timeout 600

  IMPORTANT: The provision script runs helm but does NOT pass the sensitive
  ExternalSecret remoteRef values. You must either:
  (a) Pass them to provision-namespace.sh via env or extend the script, OR
  (b) Run helm directly with --set flags:

    helm upgrade --install "$E2E_NS" helm/gastown/ \
      -n "$E2E_NS" --create-namespace \
      --values helm/gastown/values.yaml \
      --values helm/gastown/values-e2e.yaml \
      --set bd-daemon.externalSecrets.doltRootPassword.remoteRef=shared-e2e-dolt-root-password \
      --set bd-daemon.externalSecrets.daemonToken.remoteRef=shared-e2e-bd-daemon-token \
      --set bd-daemon.externalSecrets.doltS3Credentials.remoteRef=shared-e2e-dolt-s3-credentials \
      --set bd-daemon.externalSecrets.slackBotCredentials.remoteRef=gastown-uat/slack-bot-credentials \
      --set bd-daemon.externalSecrets.slackBotCredentials.enabled=true \
      --timeout 600s --wait

  Store the namespace name for subsequent steps:
    export E2E_NAMESPACE="$E2E_NS"

Verify:
  All pods in the namespace reach Running or Completed state within 600s.
  Minimum expected pods: dolt (2/2), daemon (2/2), redis (1/1), nats (1/1),
  coop-broker (2/2), agent-controller (1/1).

OnFail:
  Check pod events: kubectl get events -n $E2E_NS --sort-by=.lastTimestamp
  Check pod logs for crashed containers.
  If ExternalSecrets fail, verify ClusterSecretStore and remoteRef values.
  Cleanup the failed namespace: helm uninstall $E2E_NS -n $E2E_NS; kubectl delete ns $E2E_NS
"""
needs = ["preflight"]

[[steps]]
id = "seed-config"
title = "Seed deploy config and verify daemon connectivity"
description = """
The fresh namespace has pods but the daemon needs deploy config seeded
into the Dolt config table, and Claude credentials need to be synced.

Action:
  1. Wait for the daemon pod to be ready (2/2 containers):
     kubectl wait --for=condition=ready pod -l app.kubernetes.io/component=bd-daemon \
       -n $E2E_NAMESPACE --timeout=120s

  2. Port-forward to the daemon:
     DAEMON_SVC=$(kubectl get svc -n $E2E_NAMESPACE --no-headers | grep "daemon" | grep -v dolt | head -1 | awk '{print $1}')
     kubectl port-forward -n $E2E_NAMESPACE svc/$DAEMON_SVC 19876:9876 &

  3. Verify daemon is healthy:
     curl -sf http://localhost:19080/healthz (if HTTP enabled)
     OR: bd connect --url http://localhost:19876 && bd stats

  4. Seed deploy config (Redis, NATS URLs are cluster-internal):
     kubectl exec -n $E2E_NAMESPACE <daemon-pod> -c bd-daemon -- \
       bd config set deploy.redis_url "redis://${E2E_NAMESPACE}-redis-master:6379"
     kubectl exec -n $E2E_NAMESPACE <daemon-pod> -c bd-daemon -- \
       bd config set deploy.nats_url "nats://localhost:4222"

  5. Sync Claude credentials to the coop-broker PVC:
     If using --credentialsSecret in the helm values, credentials are
     already mounted. Otherwise run sync-claude-credentials.sh.

Verify:
  Daemon responds to health checks.
  Config table has deploy.redis_url and deploy.nats_url set.
  At least one agent pod spawns within 120s of controller becoming ready.

OnFail:
  Check daemon logs: kubectl logs -n $E2E_NAMESPACE <daemon-pod> -c bd-daemon --tail=50
  Check ExternalSecret status: kubectl get externalsecret -n $E2E_NAMESPACE
"""
needs = ["provision"]

[[steps]]
id = "wait-agents"
title = "Wait for agent pods to spawn and register"
description = """
The agent controller should reconcile and spawn agent pods once the daemon
and coop-broker are healthy.

Action:
  1. Wait for at least 1 agent pod (gt-* prefix) to appear:
     for i in $(seq 1 30); do
       AGENTS=$(kubectl get pods -n $E2E_NAMESPACE --no-headers | grep "^gt-" | grep Running)
       if [ -n "$AGENTS" ]; then break; fi
       sleep 10
     done

  2. Wait for agent pods to be ready (1/1):
     kubectl wait --for=condition=ready pod -l gastown.io/role \
       -n $E2E_NAMESPACE --timeout=300s

  3. Verify agents registered with coop-broker:
     Port-forward to coop-broker and check /api/v1/pods

Verify:
  At least 1 agent pod is Running 1/1.
  Agent pod is registered with the coop-broker.
  Coop /api/v1/agent returns a valid state on the agent pod.

OnFail:
  Check controller logs: kubectl logs -n $E2E_NAMESPACE -l app.kubernetes.io/component=agent-controller --tail=100
  Check agent pod events: kubectl describe pod -n $E2E_NAMESPACE -l gastown.io/role
  If no agent spawns, verify controller RBAC and daemon connectivity.
"""
needs = ["seed-config"]

[[steps]]
id = "infra-health"
title = "Phase 1: Infrastructure health validation"
description = """
Run all infrastructure health test modules against the fresh namespace.

Action:
  E2E_NAMESPACE=$E2E_NAMESPACE ./tests/e2e/scripts/run-suite.sh \
    --skip agent-spawn --skip agent-state --skip agent-io \
    --skip agent-credentials --skip agent-resume --skip agent-multi \
    --skip agent-coordination --skip agent-cleanup --skip agent-lifecycle

  This runs: dolt-health, redis-health, daemon-health, nats-health,
  coop-broker-health, controller-health, git-mirror-health,
  dolt-s3-sync, controller-failsafe, slack-bot-health.

Verify:
  All 10 infrastructure modules pass. Git-mirror may skip if not deployed.

OnFail:
  For each failed module, file a bug:
    ./tests/e2e/scripts/file-bug.sh --formula mol-gastown-e2e-full \
      --step infra-health --namespace $E2E_NAMESPACE
  Do NOT proceed to Phase 2 if critical infrastructure is broken
  (dolt, daemon, or controller failures are blocking).
"""
needs = ["wait-agents"]

[[steps]]
id = "agent-capabilities"
title = "Phase 2: Agent capability validation"
description = """
Run all agent capability test modules against the fresh namespace.

Action:
  E2E_NAMESPACE=$E2E_NAMESPACE ./tests/e2e/scripts/run-suite.sh \
    --skip dolt-health --skip redis-health --skip daemon-health \
    --skip nats-health --skip coop-broker-health --skip controller-health \
    --skip git-mirror-health --skip dolt-s3-sync --skip controller-failsafe \
    --skip slack-bot-health

  This runs: agent-spawn, agent-state, agent-io, agent-credentials,
  agent-resume, agent-multi, agent-coordination, agent-cleanup, agent-lifecycle.

Verify:
  All 9 agent modules pass (71+ tests).

OnFail:
  For each failed module, file a bug:
    ./tests/e2e/scripts/file-bug.sh --formula mol-gastown-e2e-full \
      --step agent-capabilities --namespace $E2E_NAMESPACE
"""
needs = ["infra-health"]

[[steps]]
id = "mux-dashboard"
title = "Phase 3: Mux dashboard Playwright tests"
description = """
Run the Playwright mux.spec.js test suite against the coop broker mux dashboard.

Action:
  Set up port-forward to coop-broker:
    MUX_SVC=$(kubectl get svc -n $E2E_NAMESPACE --no-headers | grep coop-broker | head -1 | awk '{print $1}')
    kubectl port-forward -n $E2E_NAMESPACE svc/$MUX_SVC 18080:8080 &
    sleep 3
    cd tests/e2e && npx playwright test mux.spec.js

Verify:
  All 29 Playwright tests pass.

OnFail:
  Save Playwright traces from tests/e2e/test-results/
  File bug:
    ./tests/e2e/scripts/file-bug.sh --formula mol-gastown-e2e-full \
      --step mux-dashboard --namespace $E2E_NAMESPACE \
      --trace tests/e2e/test-results/
"""
needs = ["agent-capabilities"]

[[steps]]
id = "final-summary"
title = "E2E suite final summary"
description = """
Produce the final E2E test report.

Action:
  Summarize results from all phases:
  - Phase 1 (Infrastructure): X/10 modules passed
  - Phase 2 (Agent Capabilities): X/9 modules passed
  - Phase 3 (Mux Dashboard): X/29 Playwright tests passed
  - Total: X tests passed, Y failed, Z skipped

  If any bugs were filed, list them with IDs and titles.
  Report overall: PASS or FAIL.

  Optionally run the full suite for a unified JSON report:
    E2E_NAMESPACE=$E2E_NAMESPACE ./tests/e2e/scripts/run-suite.sh --json

Verify:
  Summary is complete and accurate.
"""
needs = ["infra-health", "agent-capabilities", "mux-dashboard"]

[[steps]]
id = "teardown"
title = "Teardown: delete E2E namespace"
description = """
Clean up the ephemeral namespace and all its resources.

Action:
  1. Uninstall the helm release:
     helm uninstall "$E2E_NAMESPACE" -n "$E2E_NAMESPACE"

  2. Delete the namespace (this cleans up PVCs, secrets, etc.):
     kubectl delete namespace "$E2E_NAMESPACE" --wait=true --timeout=120s

  3. Verify cleanup:
     kubectl get ns "$E2E_NAMESPACE" 2>&1 | grep -q "not found"

  4. If any PVCs or PVs persist (EBS volumes), clean them up:
     kubectl get pv | grep "$E2E_NAMESPACE" | awk '{print $1}' | \
       xargs -r kubectl delete pv

Verify:
  Namespace is gone. No orphaned PVs remain.

OnFail:
  Namespace deletion can hang if finalizers are stuck. Force if needed:
    kubectl patch ns $E2E_NAMESPACE -p '{"metadata":{"finalizers":[]}}' --type=merge
  List any remaining resources:
    kubectl api-resources --verbs=list -o name | xargs -n 1 kubectl get -n $E2E_NAMESPACE 2>/dev/null | grep -v "No resources"
"""
needs = ["final-summary"]

# ── Variables ────────────────────────────────────────────────────────
# No required variables — the formula generates its own ephemeral namespace.
# Optional overrides for testing against an existing namespace.

[vars]

[vars.chart_dir]
description = "Path to the gastown helm chart (default: helm/gastown)"
required = false

[vars.values_file]
description = "Path to the E2E values overlay (default: helm/gastown/values-e2e.yaml)"
required = false

[vars.skip_provision]
description = "Set to 'true' to skip provisioning and use an existing namespace"
required = false

[vars.skip_teardown]
description = "Set to 'true' to keep the namespace alive after tests (for debugging)"
required = false

[vars.namespace]
description = "Override namespace name (default: gastown-e2e-<timestamp>). If provided with skip_provision=true, tests run against this existing namespace."
required = false
